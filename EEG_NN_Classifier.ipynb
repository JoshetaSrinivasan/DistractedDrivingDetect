{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is modified from \"http://learn.neurotechedu.com/machinelearning/\" \n",
    "\n",
    "# Turn off warnings \n",
    "# You'll want to comment this out if you plan on modifying this code, to get valuable feedback\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from collections import OrderedDict\n",
    "from pylab import rcParams\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import csv\n",
    "import random\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment below if needed: \n",
    "# # Set the randomizer seed for some consistency\n",
    "# torch.manual_seed(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network \n",
    "Our Neural Network is going to be a simple Multi-Layer-Perceptron(MLP). \n",
    "So what is a MLP? Here are some resources to give you an understanding: \n",
    "1. https://machinelearningmastery.com/neural-networks-crash-course/\n",
    "2. https://www.youtube.com/watch?v=u5GAVdLQyIg (literally my fav)\n",
    "\n",
    "For our MLP, we want 3 sets of EEG data combined into a single sample; passed into the NN and for the NN to return if the data corresponds to Mind-Wandering or not. So, the structure of our MLP should be as follows:\n",
    "\n",
    "Our MLP: \n",
    "24 neurons (input) -> 7 neurons (hidden layer) -> 6 neurons (hidden layer)  -> 5 neurons (output layer) -> 1(output)\n",
    "\n",
    "The rationale for the input and output sizes are obvious enough (8*3 = 24 for input; we need one output -> is this mindwandering?) Yet you may ask: \n",
    "    1. Why 2 hidden layers? \n",
    "    2. Why the number of neurons for each layer?\n",
    "\n",
    "I chose these hyper-parameters simply by trial and error. I had two things to balance. I needed sufficiently large number of neurons and layers to be able to be sensitive to Mind-Wandering. I also needed it to be sufficiently small to fit onto the Arduino Uno. This configuration was the sweet-spot I managed to hit. \n",
    "\n",
    "Let's first innitialize these parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Innitialize parameters\n",
    "eeg_sample_length = 3*8  # No. of data points in 1 sample = no. of readings * no. of EEG data points in one reading\n",
    "learning_rate = 6e-3     # How hard the network will correct its mistakes while learning\n",
    "number_of_classes = 1    # no. of binary questions we want answered: is this mindwander?\n",
    "hidden1 = 7              # Number of neurons in our first hidden layer\n",
    "hidden2 = 6              # Number of neurons in our second hidden layer\n",
    "output = 5               # Number of neurons in our output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we make the network. We are going to be using pyTorch to do so. We have imported torch.nn as nn. I.e. we can access torch.nn (the pyTorch module for Neural Networks) as nn. \n",
    "\n",
    "First we use nn.sequential to build a Neural Network model: nn.sequential takes in modules and adds them to the model in the order that they are passed in: https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html\n",
    "\n",
    "These modules can be a layers of neurons, activation functions for layers of neurons, etc. We are going to have 4 layers (in addition, to the input and output) with activation for each layer.\n",
    "\n",
    "The following block of code achieves this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the network \n",
    "\n",
    "nn_model = nn.Sequential() # Innit model \n",
    "\n",
    "# Add Input layer (Size 24 -> 7)\n",
    "nn_model.add_module('InputLinear', nn.Linear(eeg_sample_length, hidden1))\n",
    "nn_model.add_module('InputActivation', nn.ELU()) \n",
    "\n",
    "# Add Hidden layer (Size 7 -> 6)\n",
    "nn_model.add_module('HiddenLinear', nn.Linear(hidden1, hidden2))\n",
    "nn_model.add_module('HiddenActivation', nn.ReLU())\n",
    "\n",
    "# Add Hidden Layer (Size 6 -> 5)\n",
    "nn_model.add_module('HiddenLinear2', nn.Linear(hidden2, output))\n",
    "nn_model.add_module('HiddenActivation2', nn.ReLU())\n",
    "\n",
    "\n",
    "# Add Output Layer (Size 5 -> 1)\n",
    "nn_model.add_module('OutputLinear', nn.Linear(output, number_of_classes))\n",
    "nn_model.add_module('OutputActivation', nn.Sigmoid())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have out NN model, we have to define a \n",
    "1. Loss function: This function helps a NN learn. It calculates the difference between the expected output and the NN's current given output for the same input. Minimizing this is the goal of training. \n",
    "\n",
    "2. Training function: This is the function that actually trains the NN. It iterates over a large number and changes the weights in an attempt to minimize the output of the loss function. The goal is to hit loss = 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training and loss function \n",
    "\n",
    "# Loss function definition\n",
    "loss_function = torch.nn.MSELoss() # common loss function available in pyTorch\n",
    "\n",
    "# Trainig procedure definition\n",
    "def train_network(train_data, actual_class, iterations):\n",
    "    # Keep track of loss at every training iteration\n",
    "    loss_data = []\n",
    "    \n",
    "    # Begin training for a certain amount of iterations\n",
    "    for i in range(iterations):\n",
    "        # Begin with a classification\n",
    "        classification = nn_model(train_data)\n",
    "        \n",
    "        # Find out how wrong the network was\n",
    "        loss = loss_function(classification, actual_class)\n",
    "        loss_data.append(loss)\n",
    "        \n",
    "        # Zero out the optimizer gradients every iteration\n",
    "        optimizer.zero_grad() # to prevent gradient explosion\n",
    "        \n",
    "        # Teach the network how to do better next time # Update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Plot a nice loss graph at the end of training\n",
    "    rcParams['figure.figsize'] = 10, 5\n",
    "    plt.title(\"Loss vs Iterations\")\n",
    "    plt.plot(list(range(0, len(loss_data))), loss_data)\n",
    "    plt.show()\n",
    "    \n",
    "# Save the network's default state so we can retrain from the default weights\n",
    "torch.save(nn_model, \"nn_model_default_state\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "\n",
    "We now have a Network and a function that can train it. Our next step is to actually train our network. Before this can be done, however, we need to be get our data to be in a format that can be fed into the network to train it. The next couple of blocks achieve this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Retrieve data from CSV file \n",
    "data_path = \"/Users/JoshetaaMacbookPro/Desktop/BCIProject/MindWanderData/dataCollection/EEGdata_mindWander.csv\"\n",
    "# ^^ Replace with the data path of YOUR csv file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step to be able to train our network is to create two data sets: \n",
    "    1. Mind-Wandering samples (posEvent)\n",
    "    2. Non Mind-Wandering samples (negEvent)\n",
    " \n",
    "Before we do that, we need to read our file, store it in a list/array to perform computations with, and filter out what we dont need. The next block of code demonstrates this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Splitting Data into positive and negative samples \n",
    "\n",
    "# Innitialize lists\n",
    "data = []            # This will contain the data from the CSV file \n",
    "posEvents = []       # This will contain Mind-Wandering samples\n",
    "negEvents = []       # This will contain non Mind-Wandering samples\n",
    "\n",
    "\n",
    "with open(data_path, newline='') as csvfile:    # Open csv file \n",
    "    rawData = csv.reader(csvfile)\n",
    "    for row in rawData: \n",
    "        data.append(row)                        # Update data[] with the lines from CSV file\n",
    "\n",
    "del data[0]                       # Remove headers\n",
    "\n",
    "for line in range(len(data)):     # Remove signal strength, attn, and meditatn values\n",
    "    data[line] = data[line][3:]\n",
    "\n",
    "### Remove Data that have button pressess 'too close' together (i.e. within 10 readings)###\n",
    "buttonLines = []                    # Innitialize list \n",
    "for line in range(len(data)):       # Add line numebr to list if they have a \"*\" at the end\n",
    "    if data[line][-1] == \"*\":\n",
    "        buttonLines.append(line)\n",
    "        \n",
    "prevLine = buttonLines[0]          # Innitialize previous line\n",
    "for line in buttonLines[1:]:       # Go through all the button lines \n",
    "    if line-prevLine < 10:         # If lines too close together\n",
    "        del data[prevLine-5:line]  # remove the first button instance \n",
    "    prevLine = line                # reset previous line to current line\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have pre-processed our data. Now we can split the data into the two required classes: \n",
    "Mind Wandering V/S not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posEv: 73, negEv: 73 \n",
      "Pos shape:  (73, 24)\n",
      "Neg shape:  (73, 24)\n"
     ]
    }
   ],
   "source": [
    "data2 = data.copy()                          # Create a duplicate data list (data2)\n",
    "\n",
    "for line in range(len(data)-6):\n",
    "    sampleP = []   # Will contain positive samples\n",
    "    sampleN = []   # Will contain negaiive samples\n",
    "    \n",
    "    if data[line][-1] == \"*\":              # for lines with button-press\n",
    "        # POSITIVE EVENTS \n",
    "        del data[line][-1]\n",
    "        sampleP.append(data[line-5:line])  # choose 5 seconds of data before button press\n",
    "        del sampleP[0][3:5]                # delete last 2 seconds of data (reaction time)\n",
    "        # make sample a single array\n",
    "        sampleP = np.concatenate(np.squeeze(np.array(sampleP)), axis = 0) \n",
    "        sampleP = list(map(int, sampleP))  # converts elements from string to integer\n",
    "        posEvents.append(sampleP)          # add sample to posEvents list\n",
    "\n",
    "       \n",
    "        # NEGATIVE EVENTS\n",
    "        sampleN.append(data[line+1:line+6]) # choose 5 seconds of data after button press\n",
    "        del sampleN[0][0:2]                 # delete first 2 seconds of data (reaction time)\n",
    "        # make sample a single array\n",
    "        sampleN = np.concatenate(np.squeeze(np.array(sampleN)), axis = 0)\n",
    "        sampleN = list(map(int, sampleN))   # converts elements from string to integer\n",
    "        negEvents.append(sampleN)           # add sample to negEvents list\n",
    "\n",
    "\n",
    "        # Mark processed data with 0s\n",
    "        data2[line-5] = 0\n",
    "        data2[line-4] = 0\n",
    "        data2[line-3] = 0\n",
    "        data2[line-2] = 0\n",
    "        data2[line-1] = 0\n",
    "        data2[line] = 0\n",
    "        data2[line+1] = 0\n",
    "        data2[line+2] = 0\n",
    "        data2[line+3] = 0\n",
    "        data2[line+4] = 0\n",
    "        data2[line+5] = 0\n",
    "\n",
    "    \n",
    "# Prints number os pos and neg events found\n",
    "print(\"posEv: {}, negEv: {} \".format(len(posEvents), len(negEvents)))\n",
    "    \n",
    "# create arrays of the events lists \n",
    "pos = np.array(posEvents)\n",
    "neg = np.array(negEvents)\n",
    "\n",
    "# Prints shape of events arrays\n",
    "print(\"Pos shape: \", pos.shape)\n",
    "print(\"Neg shape: \",neg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have split our data into two data sets - pos and neg - we should be able to start training our Neural Network. However, for a good machine learning model (i.e. NN) we need to scale and normalize our data. Here's is a really good article that explains why: \n",
    "\n",
    "https://towardsdatascience.com/all-about-feature-scaling-bcc0ad75cb35#:~:text=Feature%20scaling%20is%20essential%20for,that%20calculate%20distances%20between%20data.&text=Therefore%2C%20the%20range%20of%20all,proportionately%20to%20the%20final%20distance.\n",
    "\n",
    "In the next block of code, I use the standard scaling functionlity in scikit learn that we have imported to scale our data between (-1,1) and also to normalize it to get a mean of 0 and variance of 1.\n",
    "\n",
    "As documented here: \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "This is done using the following formula -\n",
    "\n",
    "x = (x - mean)/standard deviation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Means:  [763727.5, 305545.2191780822, 120259.80821917808, 108257.65753424658, 107781.88356164383, 93348.26712328767, 59532.41095890411, 41665.479452054795, 774772.5342465753, 307755.4315068493, 127224.13698630137, 99825.27397260274, 119648.16438356164, 111443.3493150685, 68701.67123287672, 47535.42465753425, 670013.2602739726, 336091.51369863015, 137296.18493150684, 111801.14383561644, 130838.75342465754, 114929.1095890411, 77775.83561643836, 57266.95205479452]\n",
      "\n",
      "Scales:  [689481.9648178221, 344369.2637187367, 168157.14352828835, 164327.45929520408, 145694.50214552248, 121902.04244197278, 76709.82527794976, 60074.24470046851, 650649.7912980696, 364156.8012847353, 177017.4129349616, 129661.47426841634, 160570.00969977758, 159264.76036817252, 102066.6424713352, 64606.56944763643, 558380.868423735, 470732.73160098365, 224739.0479448347, 145396.46561295632, 177029.31711959356, 176695.24692303827, 127958.09637405365, 97400.32273555797]\n"
     ]
    }
   ],
   "source": [
    "## Standard Scalar \n",
    "eeg_data_scaler = StandardScaler()           # innit instance of scalar\n",
    "conc = np.concatenate((pos,neg), axis=0)     # Create a set of all pos and neg data joined\n",
    "scaled = eeg_data_scaler.fit_transform(conc) # Scale the set \n",
    "pos = np.split(scaled,2)[0]                  # Split scaled positive\n",
    "neg = np.split(scaled,2)[1]                  # Split scaled negative \n",
    "\n",
    "means = list(eeg_data_scaler.mean_)          # Get a list of the means used in scaling \n",
    "\n",
    "scales = list(eeg_data_scaler.scale_)        # Get a list of the s.d. used in scaling   \n",
    "\n",
    "print(\"Means: \", means)\n",
    "print()\n",
    "print(\"Scales: \", scales)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to copy the means and scales into our arduino program to scale the input data with the same scaling used in training the NN. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have scaled positive and negative event data, we need to convert them into a form that our pytorch NN model can take in as input. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data count: 74\n",
      "positive testing data count: 36\n",
      "negative testing data count: 36\n",
      "training labels count: 74\n"
     ]
    }
   ],
   "source": [
    "## Prepare the train and test tensors\n",
    "# Params\n",
    "percent = 50                                # You can change the percent of samples you want to test with\n",
    "nTest = int((percent/100)*pos.shape[0])     # Define the number of testing  samples\n",
    "nTrain = pos.shape[0]-nTest                 # Define number of training samples \n",
    "\n",
    "\n",
    "# Specify pos train and test samples\n",
    "pos_train = pos[:nTrain]                  # list of samples to train \n",
    "pos_test = pos[nTrain:]                   # list of samples to test\n",
    "pos_test = torch.tensor(pos_test).float() # Make test samples to a tensor\n",
    "\n",
    "# Specify Neg train and test samples\n",
    "neg_train = np.array(random.choices(neg, k=nTrain))  # list of samples to train \n",
    "neg_test = np.array(random.choices(neg, k=nTest))    # list of samples to test\n",
    "neg_test = torch.tensor(neg_test).float()            # Make test samples to a tensor\n",
    "\n",
    "\n",
    "# Combine everything into their final structures\n",
    "training_data = torch.tensor(np.concatenate((pos_train, neg_train), axis = 0)).float()  # Train data \n",
    "positive_testing_data = torch.tensor(pos_test).float()                                  # Test pos data \n",
    "negative_testing_data = torch.tensor(neg_test).float()                                  # Test neg data \n",
    "\n",
    "# Print the size of each of our data structures\n",
    "print(\"training data count: \" + str(training_data.shape[0]))\n",
    "print(\"positive testing data count: \" + str(positive_testing_data.shape[0]))\n",
    "print(\"negative testing data count: \" + str(negative_testing_data.shape[0]))\n",
    "\n",
    "# Generate training labels\n",
    "labels = torch.tensor(np.zeros((training_data.shape[0],1))).float()\n",
    "labels[0:nTrain+1] = 1.0\n",
    "print(\"training labels count: \" + str(labels.shape[0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1999e+00,  1.9412e+00, -4.4360e-01, -1.0592e-01, -4.4011e-01,\n",
       "         -3.3042e-01, -4.1873e-01, -5.5564e-01,  1.1092e+00,  1.1190e+00,\n",
       "         -5.7833e-01,  8.0910e-01, -5.8992e-01, -3.2458e-01, -2.6332e-01,\n",
       "         -5.7832e-01,  1.0491e+00, -2.5156e-01, -3.9398e-01, -2.5180e-02,\n",
       "         -1.6684e-01, -4.0006e-01, -3.5726e-01, -4.8050e-01],\n",
       "        [ 1.2732e-01, -7.5130e-01, -4.2242e-01, -5.1515e-01, -5.1084e-01,\n",
       "         -5.0616e-01, -6.6003e-01, -6.4912e-01, -5.9091e-01, -5.1766e-01,\n",
       "         -3.8391e-01, -2.6007e-01, -4.0460e-01, -5.1110e-01, -5.4679e-01,\n",
       "         -5.3198e-01,  6.1999e-01, -4.7841e-01,  1.1131e-01,  9.0425e-01,\n",
       "         -3.3753e-04, -1.4962e-01,  8.0984e-01,  5.5694e-01],\n",
       "        [ 1.2749e+00,  1.6907e+00,  2.5244e+00,  3.3856e+00,  2.1601e+00,\n",
       "          1.9974e+00,  2.1905e+00,  1.6628e+00,  2.3062e+00,  1.9150e+00,\n",
       "          2.3816e+00,  4.1409e-01,  2.4560e+00,  1.6489e+00,  1.3398e+00,\n",
       "          1.1826e+00, -9.9273e-02,  7.7425e-02, -5.7067e-02,  5.7552e-01,\n",
       "          2.7402e+00,  5.0744e-01, -2.8361e-01, -1.9288e-01],\n",
       "        [-2.6936e-01, -7.3283e-01, -6.4356e-01, -6.1803e-01, -3.9707e-01,\n",
       "         -2.1990e-01, -3.1007e-01, -3.0625e-01, -7.1833e-01,  9.4817e-01,\n",
       "          9.7679e-02, -3.2355e-01, -1.1383e-01,  7.7272e-01,  3.1022e-01,\n",
       "         -3.7480e-01, -5.8226e-01, -6.4185e-01, -5.4321e-01, -6.8877e-01,\n",
       "         -6.8844e-01, -6.1972e-01, -5.5273e-01, -5.4092e-01]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Want to see example samples within the testing data set? \n",
    "# Here's how you can print the first four samples in each set: \n",
    "pos_test[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.3976e-01, -1.9835e-01,  1.3929e+00,  1.0768e+00,  7.3255e-01,\n",
       "          4.5513e-01, -2.4986e-01, -2.1981e-02, -3.6425e-01, -4.0805e-01,\n",
       "         -4.3827e-01, -4.5450e-01, -2.3070e-01, -4.8950e-01, -4.2547e-01,\n",
       "         -6.1793e-01,  1.4582e+00,  6.4684e-01,  8.6561e-01,  6.4501e-01,\n",
       "          9.7399e-01,  9.1536e-01,  4.5603e-01,  2.9093e-01],\n",
       "        [-7.4448e-02, -1.4801e-01, -4.8234e-01, -3.5082e-01,  6.1228e-01,\n",
       "          3.7713e-03, -3.0629e-01, -3.5309e-01,  3.3933e-01, -1.6694e-01,\n",
       "         -4.3499e-01, -8.6828e-02, -2.2585e-01, -3.3173e-01, -1.9450e-01,\n",
       "         -6.6480e-01,  1.1144e-01, -1.9215e-01, -1.1375e-01, -1.8287e-01,\n",
       "         -3.7904e-01, -5.5006e-01, -4.2300e-01, -5.0474e-01],\n",
       "        [ 5.4546e-02, -6.6460e-01, -5.8826e-01, -4.5959e-01,  1.8627e-01,\n",
       "         -5.8288e-01, -7.0720e-01, -5.7103e-01,  1.1047e+00, -1.8635e-01,\n",
       "         -2.6167e-01,  5.6563e-01,  2.6426e-01, -2.0897e-01, -3.1460e-01,\n",
       "         -2.6913e-01,  1.2380e+00,  8.4360e-01,  1.5325e+00,  3.5653e+00,\n",
       "          2.1963e-01,  1.7043e+00,  2.8251e+00,  6.3575e-01],\n",
       "        [ 2.4788e+00,  4.1200e-01,  3.8124e+00, -7.4587e-02,  3.2401e+00,\n",
       "          5.3954e-01,  8.9162e-02,  1.0174e+00,  5.6349e-02, -5.7078e-01,\n",
       "         -6.3608e-01, -9.8906e-02,  7.0501e-01,  3.9674e-02,  4.8765e-01,\n",
       "          1.9141e-01,  7.3176e-01, -3.1116e-01,  2.6136e-01, -5.3642e-01,\n",
       "         -3.9476e-01, -5.6060e-01, -4.9061e-01, -4.8437e-01]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_test[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "We are now ready to train the model! First we load the model. Then we define the optimizer(this is used in the training function that we defined) to be a common optimization algorithm from pyTorch (Adam). See more at https://pytorch.org/docs/stable/optim.html. Then, all that's left is to call our train function that we defined!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is the loss graph for dataset training session\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## TRAINING TIME \n",
    "\n",
    "# Make sure we're starting from untrained every time\n",
    "nn_model = torch.load(\"nn_model_default_state\")\n",
    "\n",
    "# Define a learning function, needs to be reinitialized every load\n",
    "optimizer = torch.optim.Adam(nn_model.parameters(), lr = learning_rate)\n",
    "\n",
    "## Use our training procedure with the sample data\n",
    "print(\"Below is the loss graph for dataset training session\")\n",
    "train_network(training_data, labels, iterations = 1500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "We are now ready to test the model. We just pass each one of our samples in the test data set through our trained network and visualize the results. We can also evaluate for model accuracy by dividing the number of correct classifications over all classifications in the testing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Classification 1: 0.00%\n",
      "Positive Classification 2: 0.00%\n",
      "Positive Classification 3: 99.88%\n",
      "Positive Classification 4: 99.91%\n",
      "Positive Classification 5: 0.00%\n",
      "Positive Classification 6: 100.00%\n",
      "Positive Classification 7: 0.00%\n",
      "Positive Classification 8: 100.00%\n",
      "Positive Classification 9: 74.52%\n",
      "Positive Classification 10: 0.01%\n",
      "Positive Classification 11: 99.97%\n",
      "Positive Classification 12: 99.89%\n",
      "Positive Classification 13: 100.00%\n",
      "Positive Classification 14: 0.00%\n",
      "Positive Classification 15: 0.00%\n",
      "Positive Classification 16: 0.33%\n",
      "Positive Classification 17: 99.99%\n",
      "Positive Classification 18: 0.00%\n",
      "Positive Classification 19: 0.00%\n",
      "Positive Classification 20: 99.99%\n",
      "Positive Classification 21: 96.29%\n",
      "Positive Classification 22: 96.13%\n",
      "Positive Classification 23: 100.00%\n",
      "Positive Classification 24: 0.00%\n",
      "Positive Classification 25: 100.00%\n",
      "Positive Classification 26: 80.58%\n",
      "Positive Classification 27: 66.66%\n",
      "Positive Classification 28: 66.66%\n",
      "Positive Classification 29: 99.88%\n",
      "Positive Classification 30: 100.00%\n",
      "Positive Classification 31: 98.39%\n",
      "Positive Classification 32: 100.00%\n",
      "Positive Classification 33: 2.62%\n",
      "Positive Classification 34: 100.00%\n",
      "Positive Classification 35: 0.00%\n",
      "Positive Classification 36: 100.00%\n",
      "\n",
      "Negative Classification 1: 0.02%\n",
      "Negative Classification 2: 0.00%\n",
      "Negative Classification 3: 0.00%\n",
      "Negative Classification 4: 100.00%\n",
      "Negative Classification 5: 99.75%\n",
      "Negative Classification 6: 100.00%\n",
      "Negative Classification 7: 0.00%\n",
      "Negative Classification 8: 0.00%\n",
      "Negative Classification 9: 0.00%\n",
      "Negative Classification 10: 0.00%\n",
      "Negative Classification 11: 0.00%\n",
      "Negative Classification 12: 99.91%\n",
      "Negative Classification 13: 30.18%\n",
      "Negative Classification 14: 0.02%\n",
      "Negative Classification 15: 5.76%\n",
      "Negative Classification 16: 0.00%\n",
      "Negative Classification 17: 100.00%\n",
      "Negative Classification 18: 0.00%\n",
      "Negative Classification 19: 0.00%\n",
      "Negative Classification 20: 0.05%\n",
      "Negative Classification 21: 0.00%\n",
      "Negative Classification 22: 0.00%\n",
      "Negative Classification 23: 0.00%\n",
      "Negative Classification 24: 0.00%\n",
      "Negative Classification 25: 0.00%\n",
      "Negative Classification 26: 100.00%\n",
      "Negative Classification 27: 0.00%\n",
      "Negative Classification 28: 0.00%\n",
      "Negative Classification 29: 0.00%\n",
      "Negative Classification 30: 100.00%\n",
      "Negative Classification 31: 0.00%\n",
      "Negative Classification 32: 0.07%\n",
      "Negative Classification 33: 0.00%\n",
      "Negative Classification 34: 66.66%\n",
      "Negative Classification 35: 0.03%\n",
      "Negative Classification 36: 0.00%\n",
      "\n",
      "Classification Accuracy: 70.83%\n"
     ]
    }
   ],
   "source": [
    "## TESTING TIME \n",
    "# Parameters \n",
    "corr = 0       # tracks number of correct classifications\n",
    "thresh = 0.5   # threshold to classify as 'Mind-Wandering'\n",
    "# if output is greater than 0.5, we reagrd it as a Mind-Wandering classification\n",
    "\n",
    "# Classify our positive test dataset and print the results\n",
    "classification_1 = nn_model(positive_testing_data)\n",
    "for index, value in enumerate(classification_1.data.tolist()):\n",
    "    print(\"Positive Classification {1}: {0:.2f}%\".format(value[0] * 100, index + 1))\n",
    "    # if output > 0.5; we have correctly classified mind-wandering\n",
    "    if((value[0]) > thresh):   \n",
    "        corr += 1\n",
    "\n",
    "print()\n",
    "\n",
    "# Classify our negative test dataset and print the results\n",
    "classification_2 = nn_model(negative_testing_data)\n",
    "for index, value in enumerate(classification_2.data.tolist()):\n",
    "    print(\"Negative Classification {1}: {0:.2f}%\".format(value[0] * 100, index + 1))\n",
    "    # if output > 0.5; we have correctly classified non-mind-wandering\n",
    "    if((value[0]) < thresh):\n",
    "        corr += 1\n",
    "\n",
    "accuracy = (corr/(positive_testing_data.shape[0]+negative_testing_data.shape[0])) * 100\n",
    "\n",
    "print(\"\\nClassification Accuracy: {:.2f}%\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification accurcay for this run seems to be about 70.83%. This is by no means perfect but it IS better than random chance. That is, our network IS predicting something. In fact working with the super rudimentary data we have (we only have 1 reading per second and are only using 3 readings as input) + low number of samples (we have less than a 100 training samples which is pretty low) used to train the network, the network seems to do a pretty good job. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Parameters \n",
    "\n",
    "Let's say I'm satisfies with this network (if not, I might re-run the optimzation or tweak some stuff - like collecting more data, tweaking NN architecture, etc), it's time for me to implement this network on the arduino. To do so, I need to know all the weights and biases of the network. The follwing blocks of code prints this out to the screen: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: Bias\n",
      "Parameter containing:\n",
      "tensor([ 0.7652,  0.4418,  0.0502,  0.1666, -0.1359, -0.0635, -0.0716],\n",
      "       requires_grad=True)\n",
      "\n",
      "Layer 2: Bias\n",
      "Parameter containing:\n",
      "tensor([ 0.0346,  0.2822,  0.0110, -0.0268,  1.0951,  0.3666],\n",
      "       requires_grad=True)\n",
      "\n",
      "Layer 3: Bias\n",
      "Parameter containing:\n",
      "tensor([ 0.4833,  0.2136, -0.0010, -0.1560,  0.0135], requires_grad=True)\n",
      "\n",
      "Layer 4: Bias\n",
      "Parameter containing:\n",
      "tensor([0.1967], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"Layer 1: Bias\")\n",
    "print(nn_model.InputLinear.bias)\n",
    "\n",
    "print(\"\\nLayer 2: Bias\")\n",
    "print(nn_model.HiddenLinear.bias)\n",
    "\n",
    "print(\"\\nLayer 3: Bias\")\n",
    "print(nn_model.HiddenLinear2.bias)\n",
    "\n",
    "print(\"\\nLayer 4: Bias\")\n",
    "print(nn_model.OutputLinear.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: Weights ==> torch.Size([24, 7])\n",
      "tensor([[ 0.6042,  0.0423, -0.7771, -0.1065, -0.0025, -0.0631,  0.0830],\n",
      "        [-0.1354,  0.0897,  0.0396, -0.0966, -0.0243,  0.2848,  0.1619],\n",
      "        [-0.2961, -0.3912, -0.3570, -0.4011,  0.3509, -1.3397,  0.3178],\n",
      "        [ 0.1188,  0.0773,  0.0053, -0.1069,  0.2168, -0.3470, -0.0784],\n",
      "        [-0.1262,  0.3946,  0.1977,  0.4433, -0.9644,  0.4296, -0.5130],\n",
      "        [-0.8399, -0.4171,  0.0539, -0.2158, -0.1605, -0.6159, -0.0526],\n",
      "        [ 0.2613,  0.1234,  0.3681,  0.6077, -0.5284, -1.1427, -0.4470],\n",
      "        [ 0.0126, -0.0222, -0.0751,  0.2861, -0.3355, -0.8091, -0.3862],\n",
      "        [ 0.0138,  0.0295,  0.3364,  0.5084, -0.4201,  0.5862, -0.1314],\n",
      "        [ 0.4027,  0.4452, -0.2937, -0.2297,  0.2092, -0.0332, -0.2166],\n",
      "        [-0.0416,  0.1461, -0.1552, -0.2138,  0.1558, -0.0674,  0.0371],\n",
      "        [ 0.4409,  0.3429, -0.2447, -0.1197,  0.0650, -0.1115, -0.1125],\n",
      "        [ 0.0475,  0.1692,  0.0857, -0.1247,  0.0583,  0.2026, -0.4957],\n",
      "        [ 0.0926,  0.0389,  0.1163, -0.2194, -0.1412,  0.1865, -0.3740],\n",
      "        [-0.1464,  0.0877,  0.1559, -0.0229,  0.0903,  0.1022, -0.0523],\n",
      "        [ 0.1582, -0.0496, -0.1212, -0.3429,  0.2112, -0.4697,  0.1539],\n",
      "        [ 0.2555, -1.1359,  0.5846,  0.0927, -0.2101,  0.5146, -0.3305],\n",
      "        [ 0.5313,  0.7118, -0.6165, -0.7851,  0.7043,  0.6643,  0.8433],\n",
      "        [ 0.0800,  0.0705, -0.4411, -0.6832,  0.2975,  0.1906,  0.2362],\n",
      "        [-0.1070, -0.2050,  0.5008,  0.2702, -0.4197,  0.5871, -0.1827],\n",
      "        [-0.0917, -0.1557, -0.1801, -0.0902,  0.0707, -0.2908,  0.1245],\n",
      "        [ 0.5900,  0.4234,  0.2941,  0.0637, -0.5518,  0.8230, -0.4142],\n",
      "        [ 0.3984,  0.0595, -0.0293,  0.0575, -0.2813,  0.4383, -0.4294],\n",
      "        [ 0.0542, -0.4170,  0.3378, -0.2070, -0.2899, -0.2717, -0.1699]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Layer 2: Weights ==> torch.Size([7, 6])\n",
      "tensor([[ 0.4904, -0.8573,  0.2979,  0.5827,  0.9086, -0.8467],\n",
      "        [ 1.1179, -1.3644,  0.8105,  0.5020,  1.0053, -1.4117],\n",
      "        [-0.9641, -0.3055, -0.7408, -0.6852, -0.6985, -0.5892],\n",
      "        [-0.7407, -0.5948, -0.6776, -1.0106, -0.0806, -0.6092],\n",
      "        [ 0.5773,  0.6933,  0.5167,  0.6507, -0.1713,  0.3855],\n",
      "        [ 0.1196, -1.3742,  0.2422, -0.8798,  0.6540, -1.2923],\n",
      "        [ 0.1661,  0.2474,  0.0653,  0.7581,  0.4478,  0.4342]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Layer 3: Weights ==> torch.Size([6, 5])\n",
      "tensor([[ 0.5590, -1.0482, -1.1072,  0.8764, -1.9248],\n",
      "        [ 0.8485, -0.4033, -0.8608,  1.4899, -0.6038],\n",
      "        [-0.4430,  0.0403, -0.5427,  0.8684, -1.5899],\n",
      "        [ 0.7837, -0.2386, -0.3053,  0.9856, -1.2371],\n",
      "        [-0.4771,  0.9066,  1.5112,  0.0909,  2.2754],\n",
      "        [ 0.8247, -0.5619, -0.2288,  0.4609, -0.7927]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Layer 4: Weights ==> torch.Size([5, 1])\n",
      "tensor([[ 1.2759],\n",
      "        [-0.3867],\n",
      "        [-0.6416],\n",
      "        [ 1.3605],\n",
      "        [-2.7852]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Layer 1: Weights ==> {}\".format((torch.transpose(nn_model.InputLinear.weight,-2,1)).shape))\n",
    "print(torch.transpose(nn_model.InputLinear.weight,-2,1))\n",
    "\n",
    "print(\"\\nLayer 2: Weights ==> {}\".format((torch.transpose(nn_model.HiddenLinear.weight,-2,1)).shape))\n",
    "print(torch.transpose(nn_model.HiddenLinear.weight,-2,1))\n",
    "\n",
    "print(\"\\nLayer 3: Weights ==> {}\".format((torch.transpose(nn_model.HiddenLinear2.weight,-2,1)).shape))\n",
    "print(torch.transpose(nn_model.HiddenLinear2.weight,-2,1))\n",
    "\n",
    "print(\"\\nLayer 4: Weights ==> {}\".format((torch.transpose(nn_model.OutputLinear.weight,-2,1)).shape))\n",
    "print(torch.transpose(nn_model.OutputLinear.weight,-2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LE FIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python38164bit104e7a8a82c14d179202e71073891aed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
